{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ***\n",
    "        Author : Mohit Gupta\n",
    "        email  : mohit.gupta2jly@gmail.com\n",
    "        Mob.   : +91 9410509170\n",
    "    ***\n",
    "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    * This file focuses on 5 different types of Activation functions.\n",
    "    * we are supposed to provide every layer of NN a suitable activation function.\n",
    "\n",
    "    ** Points to be notted before assigning activation functions in network...\n",
    "    * Input layer should carry a linear activation function, as we don't want to change input values\n",
    "      and spending all other operations in wrong value. the output of the function will not be confined between any range.\n",
    "    * Always use a non-linear activation functions in hidden layers of neural network\n",
    "    * Always use Softmax loss function in output layer of the network.\n",
    "\n",
    "    * Rectified Linear Usit (ReLU) is most commonly used now a days.\n",
    "\n",
    "    * All definitions are also available in 'Keras'.\n",
    "\n",
    "    * Below, are the brief description of various activation functions along with respective source code.\n",
    "    * only dependancy is numpy, which is a package in python for scientific Computation.\n",
    "\n",
    "    * You can always create a your custom function and use it with this notebook imported for your arbitrary inputs to test  the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sigmoid\n",
    "    Problems with sigmoid are:\n",
    "    * It's outputs are not avilable in negative domain\n",
    "    * Output only lies between 0 and 1\n",
    "    * Makes training laughably slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tanh\n",
    "    * aka hyperbolic sigmoid\n",
    "    * It's a rescaling logistic sigmoid\n",
    "    * Its outputs range from -1 to 1\n",
    "    * its a better choice of activation function than Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return (1 - (x ** 2))\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU\n",
    "## Advantages over Sigmoid and Tanh\n",
    "    * No gradient vanishing problem, as Reluâ€™s gradient is constant = 1\n",
    "    * Sparsity. When W*x < 0, Relu gives 0, which means sparsity.\n",
    "    * Less calculation load. This may be least important. \n",
    "\n",
    "    * However, it may amplify the signal inside the network more than softmax and sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            for k in range(len(x[i])):\n",
    "                if x[i][k] > 0:\n",
    "                    x[i][k] = 1\n",
    "                else:\n",
    "                    x[i][k] = 0\n",
    "        return x\n",
    "    for i in range(0, len(x)):\n",
    "        for k in range(0, len(x[i])):\n",
    "            if x[i][k] > 0:\n",
    "                pass  # do nothing since it would be effectively replacing x with x\n",
    "            else:\n",
    "                x[i][k] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's get back in time... \n",
    "-----------------------------------------------------------------------------------------------\n",
    "___________________________ WARNING!!! ___________________________\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Some obsolete stuff below!\n",
    "    don't use them!\n",
    "\n",
    "     ________________________________________________\n",
    "    |    #######         ~~~~~~_         ////////   |\n",
    "    |  <[o` v o`]>     <[o` v o`]>     <[o` v o`]>  |\n",
    "    |                                               |\n",
    "    +++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ArcTan\n",
    "    * Better than logistic sigmoid and tanh due to 'S' shape in graph.\n",
    "    * Not computationally convinient to use.\n",
    "    * The values ranging between [-1, 1] are biologically incorrect,\n",
    "    * Still it tends to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return (np.cos(x) ** 2)\n",
    "    return np.arctan(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gaussian\n",
    "    * Not often used in neural networks anymore\n",
    "    * Slower to calculate one thing.\n",
    "    * It's expensive to compute exact Normal CDF probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            for k in range(0, len(x[i])):\n",
    "                x[i][k] = -2* x[i][k] * np.exp(-x[i][k] ** 2)\n",
    "    for i in range(0, len(x)):\n",
    "        for k in range(0, len(x[i])):\n",
    "            x[i][k] = np.exp(-x[i][k] ** 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Thats all for now!!!\n",
    "    * Now you must have learnt something,\n",
    "    * Feel free to write me fo any suggestions,\n",
    "      also if you want some good material on other related stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
